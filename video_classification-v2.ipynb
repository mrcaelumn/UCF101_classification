{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cdecc3-97b5-4ac6-b204-1400022582e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "from collections import deque\n",
    "import sys\n",
    "import gc\n",
    "import pickle, gzip\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, \\\n",
    "auc, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from packaging import version\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "\n",
    "\"\"\" Set Hyper parameters \"\"\"\n",
    "MAX_SEQ_LENGTH = 100\n",
    "NUM_FEATURES = 1024\n",
    "IMG_H = 128\n",
    "IMG_W = 192\n",
    "NUM_EPOCHS = 20\n",
    "IMG_CHANNELS = 3  ## Change this to 1 for grayscale.\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# set dir of files\n",
    "TRAIN_DATASET_VERSION = \"test_train03\"\n",
    "TEST_DATASET_VERSION = \"test_test03\"\n",
    "TRAIN_DATASET_PATH = TRAIN_DATASET_VERSION+\".csv\"\n",
    "TEST_DATASET_PATH = TEST_DATASET_VERSION+\".csv\"\n",
    "ROOT_DATASET_PATH = \"dataset/UCF-101/\"\n",
    "SAVED_MODEL_PATH = \"saved_model/\"\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "AUGMENTATION = False\n",
    "TRAIN_MODE = True\n",
    "GENERATE_DATASET = True\n",
    "RETRAIN_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d19092e-f77b-47cd-a5a9-b7a19b179b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_DATASET_PATH)\n",
    "test_df = pd.read_csv(TEST_DATASET_PATH)\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07881fc-4ef1-4ec9-b51b-e982421e15f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_frames(root_folder,arr,each_nth=10):\n",
    "    videos=[]\n",
    "    for j  in range(len(arr)):\n",
    "        print(np.round(100*j/len(arr),3))\n",
    "            \n",
    "        vcap=cv2.VideoCapture(root_folder+arr[j])\n",
    "        success=True\n",
    "  \n",
    "        frames=[]\n",
    "        cnt=0\n",
    "        while success:\n",
    "            try:\n",
    "                success,image=vcap.read()\n",
    "                cnt+=1\n",
    "                if cnt%each_nth==0:\n",
    "                    image=resize(image,(IMG_H,IMG_W))\n",
    "                    frames.append(image)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        videos.append(frames)\n",
    "    \n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df32b5-b2e2-43ad-9d42-036cd9bf8332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_frames(frames_arr , n=10):\n",
    "    videos=[]\n",
    "    for i in range(len(frames_arr)):\n",
    "        frames=[]\n",
    "        for t in np.linspace(0, len(frames_arr[i])-1, num=n):\n",
    "            frames.append(frames_arr[i][int(t)])\n",
    "        videos.append(frames)\n",
    "        \n",
    "    videos = np.array(videos)\n",
    "    print(videos.shape)\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c123ece-f01d-416b-9247-06629ad5cf3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d1c274-e947-4adb-a099-aed9c9770f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6953a82-0231-4102-bdf2-0803644c2581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_report(labels, predicts, target_names):\n",
    "    confusion = confusion_matrix(labels, predicts)\n",
    "    print('Confusion Matrix\\n')\n",
    "    print(confusion)\n",
    "    \n",
    "    print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(labels, predicts)))\n",
    "\n",
    "    print('Micro Precision: {:.2f}'.format(precision_score(labels, predicts, average='micro')))\n",
    "    print('Micro Recall: {:.2f}'.format(recall_score(labels, predicts, average='micro')))\n",
    "    print('Micro F1-score: {:.2f}\\n'.format(f1_score(labels, predicts, average='micro')))\n",
    "\n",
    "    print('Macro Precision: {:.2f}'.format(precision_score(labels, predicts, average='macro')))\n",
    "    print('Macro Recall: {:.2f}'.format(recall_score(labels, predicts, average='macro')))\n",
    "    print('Macro F1-score: {:.2f}\\n'.format(f1_score(labels, predicts, average='macro')))\n",
    "\n",
    "    print('Weighted Precision: {:.2f}'.format(precision_score(labels, predicts, average='weighted')))\n",
    "    print('Weighted Recall: {:.2f}'.format(recall_score(labels, predicts, average='weighted')))\n",
    "    print('Weighted F1-score: {:.2f}'.format(f1_score(labels, predicts, average='weighted')))\n",
    "\n",
    "    print('\\nClassification Report\\n')\n",
    "    print(classification_report(labels, predicts, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dc50b0-861b-4bf9-ae73-64d1df79dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_epoch_result(epochs, loss, name, model_name, colour):\n",
    "    plt.plot(epochs, loss, colour, label=name)\n",
    "#     plt.plot(epochs, disc_loss, 'b', label='Discriminator loss')\n",
    "    plt.title(name)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(model_name+ '_'+name+'_epoch_result.png')\n",
    "    plt.show()\n",
    "    \n",
    "class CustomSaver(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,\n",
    "                 model_path,\n",
    "                 n_model\n",
    "                ):\n",
    "        super(CustomSaver, self).__init__()\n",
    "        self.history = {}\n",
    "        self.epoch = []\n",
    "        self.model_path = model_path\n",
    "    \n",
    "        self.name_model = n_model\n",
    "        self.custom_loss = []\n",
    "        self.epochs_list = []\n",
    "            \n",
    "    def on_train_end(self, logs=None):\n",
    "        print(self.model_path)\n",
    "        self.model.save(self.model_path)\n",
    "        \n",
    "        plot_epoch_result(self.epochs_list, self.custom_loss, \"Loss\", self.name_model, \"g\")\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        self.epoch.append(epoch)\n",
    "        for k, v in logs.items():\n",
    "#             print(k, v)\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        self.epochs_list.append(epoch)\n",
    "        self.custom_loss.append(logs[\"loss\"])\n",
    "\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            self.model.save_weights(self.model_path)\n",
    "            print('saved for epoch',epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb14e04-369b-47ff-957d-418ade750d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede1c456-80d4-440f-b543-5365ba93841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_stage(model, df, root_dir):\n",
    "    print(\"testing start\")\n",
    "    num_samples = len(df)\n",
    "    df['video_paths'] = root_dir + df['tag'] + \"/\" + df['video_name']\n",
    "    video_paths = df[\"video_paths\"].values.tolist()\n",
    "    labels = df[\"tag\"].values\n",
    "    # labels = label_processor(labels[..., None]).numpy().flatten()\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "    predictions = []\n",
    "    name_list = []\n",
    "    # print(labels)\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        print(path)\n",
    "        frames = load_video(path)\n",
    "        frame_features, frame_mask = prepare_single_video(frames)\n",
    "        probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "        probs = np.argsort(probabilities)[::-1]\n",
    "        name_image = os.path.basename(path)\n",
    "        print(name_image)\n",
    "        predictions.append(class_vocab[probs[0]])\n",
    "        name_list.append(name_image)\n",
    "        # print(class_vocab[probs[0]], probs[0])\n",
    "        for i in probs:\n",
    "            print(f\"{class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    \n",
    "    \n",
    "    confusion_matrix_report(labels, predictions, class_vocab)\n",
    "    \n",
    "    \n",
    "    print(\"created csv for the result.\")\n",
    "    with open('predictions_result.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['ImageName', 'Label'])\n",
    "        writer.writerows(zip(name_list, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d457f-8c7b-48e0-9c48-b394fc440a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility for running experiments.\n",
    "def run_experiment():\n",
    "    name_model = str(IMG_SIZE)+\"_UCF101_\"+str(NUM_EPOCHS)\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "    \n",
    "    seq_model = build_our_model(len(class_vocab))\n",
    "    \n",
    "    # checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    #     SAVED_MODEL_PATH, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    # )\n",
    "    \n",
    "    path_model = SAVED_MODEL_PATH + name_model + \"_model\" + \".h5\"\n",
    "    print(path_model)\n",
    "    saver_callback = CustomSaver(\n",
    "            path_model,\n",
    "            name_model\n",
    "        )\n",
    "    \n",
    "    if RETRAIN_MODEL:\n",
    "        print(\"Model Load Weights.\")\n",
    "        # seq_model.load_weights(path_model)\n",
    "        seq_model = tf.keras.models.load_model(path_model)\n",
    "    \n",
    "    if TRAIN_MODE: \n",
    "        history = seq_model.fit(\n",
    "            [train_data[0], train_data[1]],\n",
    "            train_labels,\n",
    "            # validation_split=0.2,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            callbacks=[\n",
    "                # checkpoint, \n",
    "                saver_callback],\n",
    "        )\n",
    "    # seq_model.load_weights(SAVED_MODEL_PATH)\n",
    "    \n",
    "    seq_model = tf.keras.models.load_model(path_model)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a8255-4e20-4698-9071-26d17cc49458",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"run experiments\")\n",
    "    sequence_model = run_experiment()\n",
    "    testing_stage(sequence_model, test_df, ROOT_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120eafda-9bf6-4d3c-b555-8cc80f790008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
