{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cdecc3-97b5-4ac6-b204-1400022582e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "from collections import deque\n",
    "import sys\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, \\\n",
    "auc, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from packaging import version\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "\n",
    "\"\"\" Set Hyper parameters \"\"\"\n",
    "MAX_SEQ_LENGTH = 100\n",
    "NUM_FEATURES = 2048\n",
    "IMG_SIZE = 224\n",
    "NUM_EPOCHS = 20\n",
    "IMG_CHANNELS = 3  ## Change this to 1 for grayscale.\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# set dir of files\n",
    "TRAIN_DATASET_VERSION = \"train02\"\n",
    "TEST_DATASET_VERSION = \"test02\"\n",
    "TRAIN_DATASET_PATH = TRAIN_DATASET_VERSION+\".csv\"\n",
    "TEST_DATASET_PATH = TEST_DATASET_VERSION+\".csv\"\n",
    "ROOT_DATASET_PATH = \"dataset/UCF-101/\"\n",
    "SAVED_MODEL_PATH = \"saved_model/\"\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "AUGMENTATION = False\n",
    "TRAIN_MODE = False\n",
    "GENERATE_DATASET = True\n",
    "RETRAIN_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d19092e-f77b-47cd-a5a9-b7a19b179b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_DATASET_PATH)\n",
    "test_df = pd.read_csv(TEST_DATASET_PATH)\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07881fc-4ef1-4ec9-b51b-e982421e15f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def crop_center_square(frame):\n",
    "#     y, x = frame.shape[0:2]\n",
    "#     min_dim = min(y, x)\n",
    "#     start_x = (x // 2) - (min_dim // 2)\n",
    "#     start_y = (y // 2) - (min_dim // 2)\n",
    "#     return frame[start_y : start_y+min_dim, start_x : start_x+min_dim]\n",
    "\n",
    "# Following method is modified from this tutorial:\n",
    "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def build_feature_extractor():\n",
    "    feature_extractor = tf.keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, IMG_CHANNELS),\n",
    "    )\n",
    "    preprocess_input = tf.keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = tf.keras.Input((IMG_SIZE, IMG_SIZE, IMG_CHANNELS))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return tf.keras.Model(inputs, outputs, name=\"feature_extractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df32b5-b2e2-43ad-9d42-036cd9bf8332",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c123ece-f01d-416b-9247-06629ad5cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label preprocessing with StringLookup.\n",
    "label_processor = tf.keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None\n",
    ")\n",
    "print(label_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d1c274-e947-4adb-a099-aed9c9770f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    df['video_paths'] = root_dir + df['tag'] + \"/\" + df['video_name']\n",
    "    video_paths = df[\"video_paths\"].values.tolist()\n",
    "    # video_paths = df[\"video_name\"].values.tolist()\n",
    "    labels = df[\"tag\"].values\n",
    "    # print(labels)\n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "\n",
    "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
    "    # masked with padding or not.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx,path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.hike intern\n",
    "        #path = video_paths[idx]\n",
    "        frames = load_video(path)\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_featutes = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            try:\n",
    "                video_length = batch.shape[1]\n",
    "                length = min(MAX_SEQ_LENGTH, video_length)\n",
    "                for j in range(length):\n",
    "                    temp_frame_featutes[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "                temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "                frame_features[idx,] = temp_frame_featutes.squeeze()\n",
    "                frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "            except:\n",
    "                #print(i, j, length)\n",
    "                pass\n",
    "\n",
    "        gc.collect()\n",
    "        print(idx)\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "if GENERATE_DATASET:\n",
    "    train_data, train_labels= prepare_all_videos(train_df, ROOT_DATASET_PATH)\n",
    "    with open(TRAIN_DATASET_VERSION+'_data.pkl','wb') as f:\n",
    "        pickle.dump(train_data, f)\n",
    "\n",
    "    with open(TRAIN_DATASET_VERSION+'_labels.pkl','wb') as f:\n",
    "        pickle.dump(train_labels, f)\n",
    "        \n",
    "        \n",
    "    test_data, test_labels= prepare_all_videos(test_df, ROOT_DATASET_PATH)\n",
    "    with open(TEST_DATASET_VERSION+'_data.pkl','wb') as f:\n",
    "        pickle.dump(test_data, f)\n",
    "\n",
    "    with open(TEST_DATASET_VERSION+'_labels.pkl','wb') as f:\n",
    "        pickle.dump(test_labels, f)\n",
    "\n",
    "\n",
    "\n",
    "f = open(TRAIN_DATASET_VERSION+'_data.pkl', 'rb')\n",
    "train_data = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(TRAIN_DATASET_VERSION+'_labels.pkl', 'rb')\n",
    "train_labels = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(TEST_DATASET_VERSION+'_data.pkl', 'rb')\n",
    "test_data = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(TEST_DATASET_VERSION+'_labels.pkl', 'rb')\n",
    "test_labels = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6953a82-0231-4102-bdf2-0803644c2581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_report(labels, predicts, target_names):\n",
    "    confusion = confusion_matrix(labels, predicts)\n",
    "    print('Confusion Matrix\\n')\n",
    "    print(confusion)\n",
    "    \n",
    "    print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(labels, predicts)))\n",
    "\n",
    "    print('Micro Precision: {:.2f}'.format(precision_score(labels, predicts, average='micro')))\n",
    "    print('Micro Recall: {:.2f}'.format(recall_score(labels, predicts, average='micro')))\n",
    "    print('Micro F1-score: {:.2f}\\n'.format(f1_score(labels, predicts, average='micro')))\n",
    "\n",
    "    print('Macro Precision: {:.2f}'.format(precision_score(labels, predicts, average='macro')))\n",
    "    print('Macro Recall: {:.2f}'.format(recall_score(labels, predicts, average='macro')))\n",
    "    print('Macro F1-score: {:.2f}\\n'.format(f1_score(labels, predicts, average='macro')))\n",
    "\n",
    "    print('Weighted Precision: {:.2f}'.format(precision_score(labels, predicts, average='weighted')))\n",
    "    print('Weighted Recall: {:.2f}'.format(recall_score(labels, predicts, average='weighted')))\n",
    "    print('Weighted F1-score: {:.2f}'.format(f1_score(labels, predicts, average='weighted')))\n",
    "\n",
    "    print('\\nClassification Report\\n')\n",
    "    print(classification_report(labels, predicts, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dc50b0-861b-4bf9-ae73-64d1df79dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_epoch_result(epochs, loss, name, model_name, colour):\n",
    "    plt.plot(epochs, loss, colour, label=name)\n",
    "#     plt.plot(epochs, disc_loss, 'b', label='Discriminator loss')\n",
    "    plt.title(name)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(model_name+ '_'+name+'_epoch_result.png')\n",
    "    plt.show()\n",
    "    \n",
    "class CustomSaver(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,\n",
    "                 model_path,\n",
    "                 n_model\n",
    "                ):\n",
    "        super(CustomSaver, self).__init__()\n",
    "        self.history = {}\n",
    "        self.epoch = []\n",
    "        self.model_path = model_path\n",
    "    \n",
    "        self.name_model = n_model\n",
    "        self.custom_loss = []\n",
    "        self.epochs_list = []\n",
    "            \n",
    "    def on_train_end(self, logs=None):\n",
    "        print(self.model_path)\n",
    "        self.model.save(self.model_path)\n",
    "        \n",
    "        plot_epoch_result(self.epochs_list, self.custom_loss, \"Loss\", self.name_model, \"g\")\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        self.epoch.append(epoch)\n",
    "        for k, v in logs.items():\n",
    "#             print(k, v)\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        self.epochs_list.append(epoch)\n",
    "        self.custom_loss.append(logs[\"loss\"])\n",
    "\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            self.model.save_weights(self.model_path)\n",
    "            print('saved for epoch',epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb14e04-369b-47ff-957d-418ade750d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_our_model(nb_classes):\n",
    "    \n",
    "    frame_features_input = tf.keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = tf.keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
    "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
    "    x = tf.keras.layers.LSTM(200, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "\n",
    "    x = tf.keras.layers.LSTM(200, return_sequences=True)(x)\n",
    "\n",
    "    x = tf.keras.layers.GRU(20)(x)\n",
    "    #x = keras.layers.Dropout(0.4)(x)\n",
    "\n",
    "\n",
    "    x = tf.keras.layers.Dense(2048, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dense(1024, activation=\"relu\")(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(nb_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = tf.keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede1c456-80d4-440f-b543-5365ba93841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frames = load_video(os.path.join(\"test\", path))\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    return frames\n",
    "\n",
    "def testing_stage(model, df, root_dir):\n",
    "    print(\"testing start\")\n",
    "    num_samples = len(df)\n",
    "    df['video_paths'] = root_dir + df['tag'] + \"/\" + df['video_name']\n",
    "    video_paths = df[\"video_paths\"].values.tolist()\n",
    "    labels = df[\"tag\"].values\n",
    "    # labels = label_processor(labels[..., None]).numpy().flatten()\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "    predictions = []\n",
    "    name_list = []\n",
    "    # print(labels)\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        print(path)\n",
    "        frames = load_video(path)\n",
    "        frame_features, frame_mask = prepare_single_video(frames)\n",
    "        probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "        probs = np.argsort(probabilities)[::-1]\n",
    "        name_image = os.path.basename(path)\n",
    "        print(name_image)\n",
    "        predictions.append(class_vocab[probs[0]])\n",
    "        name_list.append(name_image)\n",
    "        # print(class_vocab[probs[0]], probs[0])\n",
    "        for i in probs:\n",
    "            print(f\"{class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    \n",
    "    \n",
    "    confusion_matrix_report(labels, predictions, class_vocab)\n",
    "    \n",
    "    \n",
    "    print(\"created csv for the result.\")\n",
    "    with open('predictions_result.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['ImageName', 'Label'])\n",
    "        writer.writerows(zip(name_list, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d457f-8c7b-48e0-9c48-b394fc440a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility for running experiments.\n",
    "def run_experiment():\n",
    "    name_model = str(IMG_SIZE)+\"_UCF101_\"+str(NUM_EPOCHS)\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "    \n",
    "    seq_model = build_our_model(len(class_vocab))\n",
    "    \n",
    "    # checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    #     SAVED_MODEL_PATH, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    # )\n",
    "    \n",
    "    path_model = SAVED_MODEL_PATH + name_model + \"_model\" + \".h5\"\n",
    "    print(path_model)\n",
    "    saver_callback = CustomSaver(\n",
    "            path_model,\n",
    "            name_model\n",
    "        )\n",
    "    \n",
    "    if RETRAIN_MODEL:\n",
    "        print(\"Model Load Weights.\")\n",
    "        # seq_model.load_weights(path_model)\n",
    "        seq_model = tf.keras.models.load_model(path_model)\n",
    "    \n",
    "    if TRAIN_MODE: \n",
    "        history = seq_model.fit(\n",
    "            [train_data[0], train_data[1]],\n",
    "            train_labels,\n",
    "            # validation_split=0.2,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            callbacks=[\n",
    "                # checkpoint, \n",
    "                saver_callback],\n",
    "        )\n",
    "    # seq_model.load_weights(SAVED_MODEL_PATH)\n",
    "    \n",
    "    seq_model = tf.keras.models.load_model(path_model)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a8255-4e20-4698-9071-26d17cc49458",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"run experiments\")\n",
    "    sequence_model = run_experiment()\n",
    "    testing_stage(sequence_model, test_df, ROOT_DATASET_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
